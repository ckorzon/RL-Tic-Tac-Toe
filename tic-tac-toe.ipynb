{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tic-tac-toe environment\n",
    "\n",
    "class TicTacToeEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        # The game board is a 2-dimensional 3x3 array\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        # There are 9 discreet actions, corresponding to placing a token in one of the nine spaces\n",
    "        self.action_space = gym.spaces.Discrete(9)\n",
    "        # The game board as an observation_space\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(3, 3), dtype=int)\n",
    "        self.current_player = 1\n",
    "        self.winner = None\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1\n",
    "        self.winner = None\n",
    "        self.done = False\n",
    "        return self.board\n",
    "\n",
    "    # Docs: https://www.gymlibrary.dev/api/core/\n",
    "    def step(self, action):\n",
    "        \"\"\"Performs a single step in a RL Episode. Takes an action as an argument, returns tuple of\n",
    "        format: (observation, reward, terminated, truncated, info)\"\"\"\n",
    "        if self.done:\n",
    "            return self.board, 0, True, {}\n",
    "\n",
    "        # Translate action number into row & column\n",
    "        row, col = divmod(action, 3)\n",
    "        # If that box is unoccupied, place token. Else, give penalty to agent.\n",
    "        if self.board[row][col] == 0:\n",
    "            self.board[row][col] = self.current_player\n",
    "        else:\n",
    "            self.current_player = -self.current_player\n",
    "            return self.board, -1, False, {}\n",
    "\n",
    "        self.winner = self.check_winner()\n",
    "\n",
    "        # Case 1: Game is over and there is a winner\n",
    "        if self.winner is not None:\n",
    "            self.done = True\n",
    "            if self.winner == self.current_player:\n",
    "                return self.board, 1, True, {}  # Big reward for winning\n",
    "            return self.board, -0.25, True, {} # This case should not be possible (opponent winning on player's turn)\n",
    "            \n",
    "        # Case 2: Game is over and there is no winner\n",
    "        elif np.all(self.board != 0):\n",
    "            self.done = True\n",
    "            return self.board, 0, True, {}\n",
    "        \n",
    "        # Case 3: Game is still going\n",
    "        else:\n",
    "            self.current_player = -self.current_player\n",
    "            return self.board, 0, False, {}\n",
    "\n",
    "    def check_winner(self):\n",
    "        for player in [-1, 1]:\n",
    "            for i in range(3):\n",
    "                if np.all(self.board[i, :] == player) or np.all(self.board[:, i] == player):\n",
    "                    return player\n",
    "            if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n",
    "                return player\n",
    "        return None\n",
    "\n",
    "    def render(self):\n",
    "        for row in self.board:\n",
    "            print(\" \".join([[\" \", \"O\", \"X\"][cell] for cell in row]))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    def get_state_string(self):\n",
    "        return self.to_state_string(self.board)\n",
    "    \n",
    "    def to_state_string(self, state):\n",
    "        state_str = \"\"\n",
    "        for row in state:\n",
    "            state_str += \"\".join([[\"-\", \"O\", \"X\"][cell] for cell in row])\n",
    "        return state_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningParams:\n",
    "    \"\"\"Class to encapsulate parameters used for Q-Learning\"\"\"\n",
    "    def __init__(self, alpha: float, gamma: float, epsilon: float):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    \"\"\"Simple Q Learning values table implemented using a dictionary\"\"\"\n",
    "    \n",
    "    def __init__(self, default_q_val: float = 0, actions_range: tuple = (0,9)):\n",
    "        # Q Table implementation as a dictionary \n",
    "        self._table = {}\n",
    "        self._default = default_q_val\n",
    "        self._actions_range = actions_range\n",
    "    \n",
    "    def get(self, state, action) -> float:\n",
    "        \"\"\"Return the Q value for a given state and action\"\"\"\n",
    "        self._register(state)\n",
    "        return self._table.get(state).get(action)\n",
    "\n",
    "    def set(self, state, action, val) -> None:\n",
    "        \"\"\"Set the Q value for a given state and action\"\"\"\n",
    "        self._register(state)\n",
    "        self._table[state][action] = val\n",
    "\n",
    "    def best_action(self, state) -> int:\n",
    "        \"\"\"Find the action with the maximum Q value for a given state\"\"\"\n",
    "        self._register(state)\n",
    "        max_q = None\n",
    "        best_actions = []\n",
    "        for action in self._table.get(state, {}).keys():\n",
    "            q = self._table.get(state).get(action)\n",
    "            if max_q is None:\n",
    "                max_q = q\n",
    "                best_actions.append(action)\n",
    "            elif max_q > q: \n",
    "                continue\n",
    "            elif max_q == q:\n",
    "                best_actions.append(action)\n",
    "            else:\n",
    "                best_actions = [action]\n",
    "                max_q = q\n",
    "        if len(best_actions) == 1:\n",
    "            return best_actions[0]\n",
    "        return choice(best_actions)\n",
    "\n",
    "    def _register(self, state) -> None:\n",
    "        \"\"\"Ensure that a given state exists with the Q table. If it does not, add the entry with the default Q value.\"\"\"\n",
    "        if self._table.get(state, None) is None:\n",
    "            self._table[state] = {}\n",
    "            for action in range(self._actions_range[0], self._actions_range[1]):\n",
    "                self._table[state][action] = self._default\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self._table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AITicAgent:\n",
    "    \"\"\"Reinforcement learning agent which uses Q-Learning to determine tic-tac-toe game action\"\"\"\n",
    "    def __init__(self, action_space: gym.spaces.Discrete, q_params: QLearningParams):\n",
    "        self.action_space = action_space\n",
    "        self.q_params = q_params\n",
    "        self.q_table = QTable()\n",
    "    \n",
    "    def choose_action(self, state: str):\n",
    "        \"\"\"Choose the next action for a given board state using Q-Learning technique\"\"\"\n",
    "        if np.random.uniform(0, 1) < self.q_params.epsilon:\n",
    "            return self.action_space.sample()\n",
    "        else:\n",
    "            return self.q_table.best_action(state)\n",
    "    \n",
    "    def update_q_table(self, state: tuple, action: int, reward: int, next_state: tuple):\n",
    "        \"\"\"Update Q Table to adjust params for a given state using the best next action \"\"\"\n",
    "        # Declare parameters\n",
    "        best_next_action = self.q_table.best_action(next_state)\n",
    "        a = self.q_params.alpha\n",
    "        g = self.q_params.gamma\n",
    "        current_q = self.q_table.get(state, action)\n",
    "        next_q = self.q_table.get(next_state, best_next_action)\n",
    "\n",
    "        # Perform Q Table update calculation\n",
    "        new_q = (1 - a) * current_q + a * (reward + g * next_q)\n",
    "        self.q_table.set(state, action, new_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 1000, A's Total Reward: 0, B's Total Reward: -1\n",
      "Episode 2000, A's Total Reward: -2, B's Total Reward: -4\n",
      "Episode 3000, A's Total Reward: -1, B's Total Reward: 1\n",
      "Episode 4000, A's Total Reward: 0, B's Total Reward: -1\n",
      "Episode 5000, A's Total Reward: 1, B's Total Reward: -1\n",
      "Episode 6000, A's Total Reward: -5, B's Total Reward: -6\n",
      "Episode 7000, A's Total Reward: -2, B's Total Reward: -2\n",
      "Episode 8000, A's Total Reward: -1, B's Total Reward: -3\n",
      "Episode 9000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 10000, A's Total Reward: 1, B's Total Reward: -3\n",
      "Episode 11000, A's Total Reward: -3, B's Total Reward: 0\n",
      "Episode 12000, A's Total Reward: -1, B's Total Reward: 0\n",
      "Episode 13000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 14000, A's Total Reward: 1, B's Total Reward: -1\n",
      "Episode 15000, A's Total Reward: 1, B's Total Reward: -1\n",
      "Episode 16000, A's Total Reward: 0, B's Total Reward: -1\n",
      "Episode 17000, A's Total Reward: -1, B's Total Reward: -2\n",
      "Episode 18000, A's Total Reward: 0, B's Total Reward: 1\n",
      "Episode 19000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 20000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 21000, A's Total Reward: 0, B's Total Reward: -1\n",
      "Episode 22000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 23000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 24000, A's Total Reward: 0, B's Total Reward: 1\n",
      "Episode 25000, A's Total Reward: -1, B's Total Reward: 1\n",
      "Episode 26000, A's Total Reward: -1, B's Total Reward: -1\n",
      "Episode 27000, A's Total Reward: 0, B's Total Reward: -2\n",
      "Episode 28000, A's Total Reward: 0, B's Total Reward: -1\n",
      "Episode 29000, A's Total Reward: 1, B's Total Reward: -1\n",
      "Episode 30000, A's Total Reward: -1, B's Total Reward: 0\n",
      "Episode 31000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 32000, A's Total Reward: 0, B's Total Reward: 1\n",
      "Episode 33000, A's Total Reward: 0, B's Total Reward: -1\n",
      "Episode 34000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 35000, A's Total Reward: 0, B's Total Reward: -1\n",
      "Episode 36000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 37000, A's Total Reward: 0, B's Total Reward: 1\n",
      "Episode 38000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 39000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 40000, A's Total Reward: 1, B's Total Reward: -1\n",
      "Episode 41000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 42000, A's Total Reward: 0, B's Total Reward: -2\n",
      "Episode 43000, A's Total Reward: 0, B's Total Reward: 0\n",
      "Episode 44000, A's Total Reward: 0, B's Total Reward: 1\n",
      "Episode 45000, A's Total Reward: 0, B's Total Reward: 1\n",
      "Episode 46000, A's Total Reward: 1, B's Total Reward: 0\n",
      "Episode 47000, A's Total Reward: -1, B's Total Reward: 1\n",
      "Episode 48000, A's Total Reward: 0, B's Total Reward: 0\n",
      "Episode 49000, A's Total Reward: -1, B's Total Reward: -2\n"
     ]
    }
   ],
   "source": [
    "# Train two agents with slightly differing configuration\n",
    "env = TicTacToeEnv()\n",
    "\n",
    "# Setup the 2 agent players\n",
    "agent_a = AITicAgent(env.action_space, QLearningParams(0.1, 0.9, 0.1))\n",
    "agent_b = AITicAgent(env.action_space, QLearningParams(0.075, 0.9, 0.15))\n",
    "players = [agent_a, agent_b]\n",
    "\n",
    "# Train for some large number of episodes (games)\n",
    "# * NOTE: There are 3^9 = 19,683 possible states of the tic-tac-toe board. Using a higher number of training episodes \n",
    "# * means greater likelihood an agent understands the value of each possible state.\n",
    "num_episodes = 50000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = [0, 0]\n",
    "    player_num = 0\n",
    "\n",
    "    while not done:\n",
    "        # Current player takes a turn:\n",
    "        action = players[player_num].choose_action(env.to_state_string(state))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        players[player_num].update_q_table(env.to_state_string(state), action, reward, env.to_state_string(next_state))\n",
    "        state = next_state\n",
    "        rewards[player_num] += reward\n",
    "\n",
    "        # Switch to next player:\n",
    "        player_num = (player_num-1)*(-1)\n",
    "\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode {episode}, A's Total Reward: {rewards[0]}, B's Total Reward: {rewards[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained agent wins 89/100 games.\n"
     ]
    }
   ],
   "source": [
    "# Test a trained agent against an untrained agent:\n",
    "test_episodes = 100\n",
    "win_count = 0\n",
    "agent_c = AITicAgent(env.action_space, QLearningParams(0.1, 0.9, 0.1))\n",
    "players = [agent_a, agent_c]\n",
    "\n",
    "for _ in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    player_num = 0\n",
    "\n",
    "    while not done:\n",
    "        action = players[player_num].choose_action(env.get_state_string()) \n",
    "        state, _, done, _ = env.step(action)\n",
    "        player_num = (player_num-1)*(-1)\n",
    "\n",
    "    if env.winner == 1:  # Trained agent wins (note env stores players differently)\n",
    "        win_count += 1\n",
    "\n",
    "# Expect the trained agent to win significant majority of the games\n",
    "print(f\"Trained agent wins {win_count}/{test_episodes} games.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent A wins 57/100 games.\n",
      "Agent B wins 43/100 games.\n"
     ]
    }
   ],
   "source": [
    "# Test a trained agent against an untrained agent:\n",
    "test_episodes = 100\n",
    "win_count = 0\n",
    "players = [agent_a, agent_b]\n",
    "\n",
    "for _ in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    player_num = 0\n",
    "\n",
    "    while not done:\n",
    "        action = players[player_num].choose_action(env.get_state_string()) \n",
    "        state, _, done, _ = env.step(action)\n",
    "        player_num = (player_num-1)*(-1)\n",
    "\n",
    "    if env.winner == 1:  # Agent A wins (note env stores players differently)\n",
    "        win_count += 1\n",
    "\n",
    "# Expect the winrate between the two trained agents to be similar\n",
    "print(f\"Agent A wins {win_count}/{test_episodes} games.\")\n",
    "print(f\"Agent B wins {test_episodes - win_count}/{test_episodes} games.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
